{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1828,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('model_data.csv').drop(columns = ['Unnamed: 0', 'distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'user_id', 'shift_id', 'prev_CW/SA_rate', 'status',\n",
       "       'S_create2SA_Create', 'S_Create2Start_Time', 'SA_Create2Start_Time',\n",
       "       'U_create2now', 'U_approve2now', 'prev_CW x SA_rate', 'type_RN',\n",
       "       'type_LVN+LPN', 'segmentName_d', 'areaName_houston', 'areaName_no',\n",
       "       'areaName_dfw', 'areaName_austin', 'areaName_san', 'net_pay', 'target',\n",
       "       'createdAt', 'Start_Time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1830,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardlize, dont standardlize dummy! \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.drop(['id', 'user_id', 'shift_id', 'status', 'target', 'createdAt', 'Start_Time', 'type_RN', \n",
    "                    'type_LVN+LPN', 'segmentName_d', 'areaName_houston', 'areaName_no', 'areaName_dfw', \n",
    "                    'areaName_austin', 'areaName_san'], axis=1))\n",
    "scaled_features = scaler.transform(df.drop(['id', 'user_id', 'shift_id', 'status', 'target', 'createdAt',\n",
    "                                            'Start_Time', 'type_RN', 'type_LVN+LPN', 'segmentName_d', \n",
    "                                            'areaName_houston', 'areaName_no', 'areaName_dfw', \n",
    "                                            'areaName_austin', 'areaName_san'], axis=1))\n",
    "\n",
    "# scaled features\n",
    "X = pd.DataFrame(scaled_features, columns = ['prev_CW/SA_rate', 'prev_CW x SA_rate', 'S_create2SA_Create', \n",
    "                                             'S_Create2Start_Time', 'SA_Create2Start_Time', 'U_create2now', \n",
    "                                             'U_approve2now', 'net_pay'])\n",
    "# concat with dummy\n",
    "df = pd.concat([df[['id', 'user_id', 'shift_id', 'status', 'target', 'createdAt',\n",
    "                    'Start_Time', 'type_RN', 'type_LVN+LPN', 'segmentName_d', \n",
    "                    'areaName_houston', 'areaName_no', 'areaName_dfw', \n",
    "                    'areaName_austin', 'areaName_san']], X], axis = 1)\n",
    "\n",
    "# drop nas\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice df by the end of this week, for predcition output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1831,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_week = '2021-3-29'\n",
    "\n",
    "# convert to datetime for conditonal selection\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n",
    "# sort by start time -> for slicing\n",
    "df = df.sort_values(by = 'Start_Time') \n",
    "# record as realdata\n",
    "realdata = df[df['Start_Time'].apply(lambda x: x >= pd.to_datetime(end_of_week))]\n",
    "# record predction output rows, don't include it in tran test validation\n",
    "realdata_len = realdata.shape[0]\n",
    "# only keep status = confirmed\n",
    "realdata = realdata[realdata['status'] == 'confirmed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = green> Validation set: 1000 recently records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1797,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    973\n",
       "1     27\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 1797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slice, dont include realdata\n",
    "validation = df[-1000-realdata_len : -realdata_len]\n",
    "\n",
    "y_valid = validation['target']\n",
    "x_valid = validation.drop(['id','user_id', 'shift_id', 'status', 'createdAt', 'Start_Time', 'target'], axis = 1)\n",
    "\n",
    "y_valid.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test: main dataset - validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1798,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:-1000-realdata_len] # slice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1799,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a dataset that num of tar = num of non tar, use it for train test\n",
    "# import random\n",
    "# df_tar = df[df['target']==1].reset_index(drop = True)\n",
    "# df_nontar = df[df['target']==0].reset_index(drop = True)\n",
    "\n",
    "# number_of_tar = df_tar.shape[0]\n",
    "# random_indices = random.sample(range(len(df_nontar)), int(number_of_tar))\n",
    "# df_nontar = df_nontar[df_nontar.index.isin(random_indices)]\n",
    "\n",
    "# # concat\n",
    "# df = pd.concat([df_tar, df_nontar]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1800,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['id','user_id', 'shift_id', 'status', 'target', 'createdAt', 'Start_Time'], axis = 1)\n",
    "y = df['target']\n",
    "\n",
    "# set test, train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1801,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    62005\n",
       "1     4345\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 1801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 1802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "# assign less punlishment for classifying 0 as 1 -> find more 1's\n",
    "# weights = {0:1, 1:10}\n",
    "# class_weight = 'balanced': automatically adjust weights inversely proportional to class frequencies in the input data\n",
    "logit = LogisticRegression(solver = 'lbfgs', class_weight = 'balanced')\n",
    "logit.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1803,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11553  7072]\n",
      " [  515   765]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.62      0.75     18625\n",
      "           1       0.10      0.60      0.17      1280\n",
      "\n",
      "    accuracy                           0.62     19905\n",
      "   macro avg       0.53      0.61      0.46     19905\n",
      "weighted avg       0.90      0.62      0.72     19905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = logit.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1804,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cf_matrix import make_confusion_matrix\n",
    "# labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "# categories = ['Zero', 'One']\n",
    "# make_confusion_matrix(confusion_matrix(y_test, y_pred), \n",
    "#                       group_names=labels,\n",
    "#                       categories=categories, \n",
    "#                       cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1805,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.235334\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>target</td>      <th>  No. Observations:  </th>   <td> 46445</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td> 46429</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    15</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 28 Mar 2021</td> <th>  Pseudo R-squ.:     </th>   <td>0.03213</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>18:35:57</td>     <th>  Log-Likelihood:    </th>  <td> -10930.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -11293.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>6.412e-145</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_RN</th>              <td>   -2.6355</td> <td>    0.174</td> <td>  -15.178</td> <td> 0.000</td> <td>   -2.976</td> <td>   -2.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_LVN+LPN</th>         <td>   -1.1000</td> <td>    0.080</td> <td>  -13.709</td> <td> 0.000</td> <td>   -1.257</td> <td>   -0.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>segmentName_d</th>        <td>    0.5200</td> <td>    0.100</td> <td>    5.202</td> <td> 0.000</td> <td>    0.324</td> <td>    0.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_houston</th>     <td>   -2.9672</td> <td>    0.095</td> <td>  -31.288</td> <td> 0.000</td> <td>   -3.153</td> <td>   -2.781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_no</th>          <td>   -2.8247</td> <td>    0.103</td> <td>  -27.418</td> <td> 0.000</td> <td>   -3.027</td> <td>   -2.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_dfw</th>         <td>   -2.9590</td> <td>    0.110</td> <td>  -26.924</td> <td> 0.000</td> <td>   -3.174</td> <td>   -2.744</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_austin</th>      <td>   -2.7655</td> <td>    0.128</td> <td>  -21.523</td> <td> 0.000</td> <td>   -3.017</td> <td>   -2.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_san</th>         <td>   -2.5709</td> <td>    0.137</td> <td>  -18.766</td> <td> 0.000</td> <td>   -2.839</td> <td>   -2.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prev_CW/SA_rate</th>      <td>    0.0653</td> <td>    0.014</td> <td>    4.595</td> <td> 0.000</td> <td>    0.037</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prev_CW x SA_rate</th>    <td>    1.4890</td> <td>    0.808</td> <td>    1.843</td> <td> 0.065</td> <td>   -0.095</td> <td>    3.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>S_create2SA_Create</th>   <td>   -2.2159</td> <td>    1.240</td> <td>   -1.787</td> <td> 0.074</td> <td>   -4.647</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>S_Create2Start_Time</th>  <td>    1.2857</td> <td>    0.923</td> <td>    1.392</td> <td> 0.164</td> <td>   -0.524</td> <td>    3.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SA_Create2Start_Time</th> <td>    0.1894</td> <td>    0.049</td> <td>    3.902</td> <td> 0.000</td> <td>    0.094</td> <td>    0.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_create2now</th>         <td>   -0.1658</td> <td>    0.049</td> <td>   -3.373</td> <td> 0.001</td> <td>   -0.262</td> <td>   -0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_approve2now</th>        <td>    0.0190</td> <td>    0.020</td> <td>    0.956</td> <td> 0.339</td> <td>   -0.020</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>net_pay</th>              <td>    0.7138</td> <td>    0.039</td> <td>   18.131</td> <td> 0.000</td> <td>    0.637</td> <td>    0.791</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   No. Observations:                46445\n",
       "Model:                          Logit   Df Residuals:                    46429\n",
       "Method:                           MLE   Df Model:                           15\n",
       "Date:                Sun, 28 Mar 2021   Pseudo R-squ.:                 0.03213\n",
       "Time:                        18:35:57   Log-Likelihood:                -10930.\n",
       "converged:                       True   LL-Null:                       -11293.\n",
       "Covariance Type:            nonrobust   LLR p-value:                6.412e-145\n",
       "========================================================================================\n",
       "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "type_RN                 -2.6355      0.174    -15.178      0.000      -2.976      -2.295\n",
       "type_LVN+LPN            -1.1000      0.080    -13.709      0.000      -1.257      -0.943\n",
       "segmentName_d            0.5200      0.100      5.202      0.000       0.324       0.716\n",
       "areaName_houston        -2.9672      0.095    -31.288      0.000      -3.153      -2.781\n",
       "areaName_no             -2.8247      0.103    -27.418      0.000      -3.027      -2.623\n",
       "areaName_dfw            -2.9590      0.110    -26.924      0.000      -3.174      -2.744\n",
       "areaName_austin         -2.7655      0.128    -21.523      0.000      -3.017      -2.514\n",
       "areaName_san            -2.5709      0.137    -18.766      0.000      -2.839      -2.302\n",
       "prev_CW/SA_rate          0.0653      0.014      4.595      0.000       0.037       0.093\n",
       "prev_CW x SA_rate        1.4890      0.808      1.843      0.065      -0.095       3.073\n",
       "S_create2SA_Create      -2.2159      1.240     -1.787      0.074      -4.647       0.215\n",
       "S_Create2Start_Time      1.2857      0.923      1.392      0.164      -0.524       3.096\n",
       "SA_Create2Start_Time     0.1894      0.049      3.902      0.000       0.094       0.285\n",
       "U_create2now            -0.1658      0.049     -3.373      0.001      -0.262      -0.069\n",
       "U_approve2now            0.0190      0.020      0.956      0.339      -0.020       0.058\n",
       "net_pay                  0.7138      0.039     18.131      0.000       0.637       0.791\n",
       "========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 1805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logit summary\n",
    "import statsmodels.api as sm\n",
    "smlogit = sm.Logit(y_train,X_train).fit()\n",
    "smlogit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26901 16479]\n",
      " [ 1203  1862]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.62      0.75     43380\n",
      "           1       0.10      0.61      0.17      3065\n",
      "\n",
      "    accuracy                           0.62     46445\n",
      "   macro avg       0.53      0.61      0.46     46445\n",
      "weighted avg       0.90      0.62      0.71     46445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_train)\n",
    "\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green> Validation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1807,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[880  93]\n",
      " [ 12  15]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       973\n",
      "           1       0.14      0.56      0.22        27\n",
      "\n",
      "    accuracy                           0.90      1000\n",
      "   macro avg       0.56      0.73      0.58      1000\n",
      "weighted avg       0.96      0.90      0.92      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test threshold\n",
    "limiter = .6\n",
    "\n",
    "y_prob = list(logit.predict_proba(x_valid)[:,1])\n",
    "y_pred = []\n",
    "count =0\n",
    "for prob in y_prob:\n",
    "    if prob >= limiter:\n",
    "        y_pred.append(1)\n",
    "        count+=1\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression 2 Not good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = green> Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_valid = validation['target']\n",
    "# x_valid = validation[['type_RN', 'type_LVN+LPN', 'areaName_houston', 'areaName_no', 'areaName_dfw',\n",
    "#                       'areaName_austin', 'areaName_san', 'net_pay',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[['type_RN', 'type_LVN+LPN', 'areaName_houston', 'areaName_no', \n",
    "#         'areaName_dfw','areaName_austin', 'areaName_san', 'net_pay',]]\n",
    "# Y = df['target']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit real data in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1810,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set input\n",
    "real_X = realdata.drop(['id','user_id', 'shift_id', 'status', 'target', 'createdAt', 'Start_Time'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1811,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concat predicted prob with data\n",
    "realdata['prob'] = list(logit.predict_proba(real_X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1812,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record when this prediction is ran\n",
    "from datetime import date\n",
    "time = str(date.today().year) + '-' + str(date.today().month) + '-' + str(date.today().day)\n",
    "\n",
    "realdata[['id', 'Start_Time', 'prob']].to_csv('pred_{}.csv'.format(time), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check real data in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1787,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisweek = pd.read_csv('download.csv')\n",
    "thisweek = thisweek[thisweek.apply(lambda row: (row['prevStatus'] == 'confirmed') or (row['status'] == 'confirmed'), axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1363,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata_test = realdata[['id', 'prob']]\n",
    "thisweek_validation = thisweek.merge(realdata_test, on = 'id', how = 'left')\n",
    "thisweek_validation.dropna(subset=['prob'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prob    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thisweek_validation[['prob']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1365,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisweek_validation\n",
    "\n",
    "def CW_by_nurse(row):\n",
    "    if row['status']=='withdrawn' and row['prevStatus'] == 'confirmed':\n",
    "        if row['withdrawnInfo_value'] == 'nurse':\n",
    "            if row['CW_Time2Start_Time'] < 0 and row['CW_Time2Start_Time'] >= -24:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "thisweek_validation['target'] = thisweek_validation.apply (lambda row: CW_by_nurse(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1366,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisweek_validation = thisweek_validation[['id', 'prob', 'target', 'net_pay', 'type', 'segmentName', 'areaName']]\n",
    "\n",
    "# thisweek_validation.to_csv('week322_check.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[328 762]\n",
      " [  3  17]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.30      0.46      1090\n",
      "           1       0.02      0.85      0.04        20\n",
      "\n",
      "    accuracy                           0.31      1110\n",
      "   macro avg       0.51      0.58      0.25      1110\n",
      "weighted avg       0.97      0.31      0.45      1110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limiter = .5\n",
    "\n",
    "\n",
    "y_prob = thisweek_validation['prob']\n",
    "y_pred = []\n",
    "count =0\n",
    "for prob in y_prob:\n",
    "    if prob >= limiter:\n",
    "        y_pred.append(1)\n",
    "        count += 1\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "        \n",
    "print(confusion_matrix(thisweek_validation['target'], y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(thisweek_validation['target'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.547651\n",
       "1    0.606011\n",
       "Name: prob, dtype: float64"
      ]
     },
     "execution_count": 1296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thisweek_validation.groupby(\"target\")['prob'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
