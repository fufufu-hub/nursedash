{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('model_data.csv').drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'user_id', 'shift_id', 'prev_CW/SA_rate', 'S_create2SA_Create',\n",
       "       'S_Create2Start_Time', 'SA_Create2Start_Time', 'U_create2now',\n",
       "       'U_approve2now', 'prev_CW x SA_rate', 'type_RN', 'type_LVN+LPN',\n",
       "       'segmentName_d', 'areaName_houston', 'areaName_no', 'areaName_dfw',\n",
       "       'areaName_austin', 'areaName_san', 'net_pay', 'target', 'createdAt',\n",
       "       'Start_Time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardlize, dont standardlize dummy! \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.drop(['id', 'user_id', 'shift_id', 'target', 'createdAt', 'Start_Time', 'type_RN', \n",
    "                    'type_LVN+LPN', 'segmentName_d', 'areaName_houston', 'areaName_no', 'areaName_dfw', \n",
    "                    'areaName_austin', 'areaName_san'], axis=1))\n",
    "scaled_features = scaler.transform(df.drop(['id', 'user_id', 'shift_id', 'target', 'createdAt',\n",
    "                                            'Start_Time', 'type_RN', 'type_LVN+LPN', 'segmentName_d', \n",
    "                                            'areaName_houston', 'areaName_no', 'areaName_dfw', \n",
    "                                            'areaName_austin', 'areaName_san'], axis=1))\n",
    "\n",
    "# scaled features\n",
    "X = pd.DataFrame(scaled_features, columns = ['prev_CW/SA_rate', 'prev_CW x SA_rate', 'S_create2SA_Create', \n",
    "                                             'S_Create2Start_Time', 'SA_Create2Start_Time', 'U_create2now', \n",
    "                                             'U_approve2now', 'net_pay'])\n",
    "# concat with dummy\n",
    "df = pd.concat([df[['id', 'user_id', 'shift_id', 'target', 'createdAt',\n",
    "                    'Start_Time', 'type_RN', 'type_LVN+LPN', 'segmentName_d', \n",
    "                    'areaName_houston', 'areaName_no', 'areaName_dfw', \n",
    "                    'areaName_austin', 'areaName_san']], X], axis = 1)\n",
    "\n",
    "# drop nas\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set future data point as realdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note !!!: real data might overlap with train test validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n",
    "realdata = df[df['Start_Time'].apply(lambda x: x >= pd.to_datetime('2021-3-22') and x < pd.to_datetime('2021-3-28'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = green> Validation set: 1000 recently records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    984\n",
       "1     16\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 1304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slice\n",
    "validation = df[-1000:]\n",
    "\n",
    "y_valid = validation['target']\n",
    "x_valid = validation.drop(['id','user_id', 'shift_id', 'createdAt', 'Start_Time', 'target'], axis = 1)\n",
    "\n",
    "y_valid.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test: main dataset - validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:-1000] # slice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataset that num of tar = num of non tar, use it for train test\n",
    "import random\n",
    "df_tar = df[df['target']==1].reset_index(drop = True)\n",
    "df_nontar = df[df['target']==0].reset_index(drop = True)\n",
    "\n",
    "number_of_tar = df_tar.shape[0]\n",
    "random_indices = random.sample(range(len(df_nontar)), int(number_of_tar))\n",
    "df_nontar = df_nontar[df_nontar.index.isin(random_indices)]\n",
    "\n",
    "# concat\n",
    "df = pd.concat([df_tar, df_nontar]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['id','user_id', 'shift_id', 'target', 'createdAt', 'Start_Time'], axis = 1)\n",
    "y = df['target']\n",
    "\n",
    "# set test, train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight={0: 1, 1: 1.5})"
      ]
     },
     "execution_count": 1308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "weights = {0:1, 1:1.5}\n",
    "logit = LogisticRegression(solver = 'lbfgs', class_weight = weights)\n",
    "logit.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 558  755]\n",
      " [ 230 1071]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.42      0.53      1313\n",
      "           1       0.59      0.82      0.69      1301\n",
      "\n",
      "    accuracy                           0.62      2614\n",
      "   macro avg       0.65      0.62      0.61      2614\n",
      "weighted avg       0.65      0.62      0.61      2614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = logit.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cf_matrix import make_confusion_matrix\n",
    "# labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "# categories = ['Zero', 'One']\n",
    "# make_confusion_matrix(confusion_matrix(y_test, y_pred), \n",
    "#                       group_names=labels,\n",
    "#                       categories=categories, \n",
    "#                       cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.643756\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>target</td>      <th>  No. Observations:  </th>   <td>  6098</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  6082</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    15</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 26 Mar 2021</td> <th>  Pseudo R-squ.:     </th>   <td>0.07125</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>12:39:24</td>     <th>  Log-Likelihood:    </th>  <td> -3925.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -4226.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.126e-118</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_RN</th>              <td>   -3.2673</td> <td>    0.282</td> <td>  -11.589</td> <td> 0.000</td> <td>   -3.820</td> <td>   -2.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_LVN+LPN</th>         <td>   -1.4330</td> <td>    0.132</td> <td>  -10.870</td> <td> 0.000</td> <td>   -1.691</td> <td>   -1.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>segmentName_d</th>        <td>    0.6277</td> <td>    0.125</td> <td>    5.035</td> <td> 0.000</td> <td>    0.383</td> <td>    0.872</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_houston</th>     <td>   -0.5028</td> <td>    0.121</td> <td>   -4.173</td> <td> 0.000</td> <td>   -0.739</td> <td>   -0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_no</th>          <td>    0.0500</td> <td>    0.134</td> <td>    0.372</td> <td> 0.710</td> <td>   -0.213</td> <td>    0.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_dfw</th>         <td>   -0.4180</td> <td>    0.143</td> <td>   -2.932</td> <td> 0.003</td> <td>   -0.697</td> <td>   -0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_austin</th>      <td>   -0.1444</td> <td>    0.174</td> <td>   -0.831</td> <td> 0.406</td> <td>   -0.485</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_san</th>         <td>    0.2883</td> <td>    0.206</td> <td>    1.401</td> <td> 0.161</td> <td>   -0.115</td> <td>    0.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prev_CW/SA_rate</th>      <td>    0.1566</td> <td>    0.028</td> <td>    5.624</td> <td> 0.000</td> <td>    0.102</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prev_CW x SA_rate</th>    <td>    1.5980</td> <td>    1.219</td> <td>    1.310</td> <td> 0.190</td> <td>   -0.792</td> <td>    3.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>S_create2SA_Create</th>   <td>   -2.5588</td> <td>    1.851</td> <td>   -1.383</td> <td> 0.167</td> <td>   -6.186</td> <td>    1.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>S_Create2Start_Time</th>  <td>    1.5712</td> <td>    1.337</td> <td>    1.175</td> <td> 0.240</td> <td>   -1.050</td> <td>    4.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SA_Create2Start_Time</th> <td>    0.1438</td> <td>    0.081</td> <td>    1.783</td> <td> 0.075</td> <td>   -0.014</td> <td>    0.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_create2now</th>         <td>   -0.0618</td> <td>    0.080</td> <td>   -0.773</td> <td> 0.439</td> <td>   -0.218</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_approve2now</th>        <td>   -0.0401</td> <td>    0.029</td> <td>   -1.370</td> <td> 0.171</td> <td>   -0.098</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>net_pay</th>              <td>    1.0726</td> <td>    0.075</td> <td>   14.269</td> <td> 0.000</td> <td>    0.925</td> <td>    1.220</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   No. Observations:                 6098\n",
       "Model:                          Logit   Df Residuals:                     6082\n",
       "Method:                           MLE   Df Model:                           15\n",
       "Date:                Fri, 26 Mar 2021   Pseudo R-squ.:                 0.07125\n",
       "Time:                        12:39:24   Log-Likelihood:                -3925.6\n",
       "converged:                       True   LL-Null:                       -4226.8\n",
       "Covariance Type:            nonrobust   LLR p-value:                1.126e-118\n",
       "========================================================================================\n",
       "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "type_RN                 -3.2673      0.282    -11.589      0.000      -3.820      -2.715\n",
       "type_LVN+LPN            -1.4330      0.132    -10.870      0.000      -1.691      -1.175\n",
       "segmentName_d            0.6277      0.125      5.035      0.000       0.383       0.872\n",
       "areaName_houston        -0.5028      0.121     -4.173      0.000      -0.739      -0.267\n",
       "areaName_no              0.0500      0.134      0.372      0.710      -0.213       0.313\n",
       "areaName_dfw            -0.4180      0.143     -2.932      0.003      -0.697      -0.139\n",
       "areaName_austin         -0.1444      0.174     -0.831      0.406      -0.485       0.196\n",
       "areaName_san             0.2883      0.206      1.401      0.161      -0.115       0.691\n",
       "prev_CW/SA_rate          0.1566      0.028      5.624      0.000       0.102       0.211\n",
       "prev_CW x SA_rate        1.5980      1.219      1.310      0.190      -0.792       3.988\n",
       "S_create2SA_Create      -2.5588      1.851     -1.383      0.167      -6.186       1.069\n",
       "S_Create2Start_Time      1.5712      1.337      1.175      0.240      -1.050       4.192\n",
       "SA_Create2Start_Time     0.1438      0.081      1.783      0.075      -0.014       0.302\n",
       "U_create2now            -0.0618      0.080     -0.773      0.439      -0.218       0.095\n",
       "U_approve2now           -0.0401      0.029     -1.370      0.171      -0.098       0.017\n",
       "net_pay                  1.0726      0.075     14.269      0.000       0.925       1.220\n",
       "========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 1311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logit summary\n",
    "import statsmodels.api as sm\n",
    "smlogit = sm.Logit(y_train,X_train).fit()\n",
    "smlogit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1274 1769]\n",
      " [ 565 2490]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.42      0.52      3043\n",
      "           1       0.58      0.82      0.68      3055\n",
      "\n",
      "    accuracy                           0.62      6098\n",
      "   macro avg       0.64      0.62      0.60      6098\n",
      "weighted avg       0.64      0.62      0.60      6098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_train)\n",
    "\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green> Validation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[640 344]\n",
      " [  1  15]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.65      0.79       984\n",
      "           1       0.04      0.94      0.08        16\n",
      "\n",
      "    accuracy                           0.66      1000\n",
      "   macro avg       0.52      0.79      0.43      1000\n",
      "weighted avg       0.98      0.66      0.78      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test threshold\n",
    "limiter = .6\n",
    "\n",
    "y_prob = list(logit.predict_proba(x_valid)[:,1])\n",
    "y_pred = []\n",
    "count =0\n",
    "for prob in y_prob:\n",
    "    if prob >= limiter:\n",
    "        y_pred.append(1)\n",
    "        count+=1\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cf_matrix import make_confusion_matrix\n",
    "# labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "# categories = ['Zero', 'One']\n",
    "# make_confusion_matrix(confusion_matrix(y_valid, y_pred), \n",
    "#                       group_names=labels,\n",
    "#                       categories=categories, \n",
    "#                       cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression 2, pick only significant vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = green> Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_valid = validation['target']\n",
    "x_valid = validation[['net_pay', 'type_RN', 'type_LVN+LPN', 'segmentName_d', 'prev_CW/SA_rate', 'areaName_houston']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['net_pay', 'type_RN', 'type_LVN+LPN', 'segmentName_d', 'prev_CW/SA_rate', 'areaName_houston']]\n",
    "Y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.654068\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>target</td>      <th>  No. Observations:  </th>   <td>  6098</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  6092</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     5</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 26 Mar 2021</td> <th>  Pseudo R-squ.:     </th>   <td>0.05637</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>12:39:24</td>     <th>  Log-Likelihood:    </th>  <td> -3988.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -4226.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>9.155e-101</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>net_pay</th>          <td>    1.1469</td> <td>    0.072</td> <td>   15.976</td> <td> 0.000</td> <td>    1.006</td> <td>    1.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_RN</th>          <td>   -3.4525</td> <td>    0.251</td> <td>  -13.754</td> <td> 0.000</td> <td>   -3.944</td> <td>   -2.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_LVN+LPN</th>     <td>   -1.7137</td> <td>    0.129</td> <td>  -13.324</td> <td> 0.000</td> <td>   -1.966</td> <td>   -1.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>segmentName_d</th>    <td>    0.5380</td> <td>    0.041</td> <td>   12.988</td> <td> 0.000</td> <td>    0.457</td> <td>    0.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prev_CW/SA_rate</th>  <td>    0.1147</td> <td>    0.025</td> <td>    4.576</td> <td> 0.000</td> <td>    0.066</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_houston</th> <td>   -0.2550</td> <td>    0.056</td> <td>   -4.563</td> <td> 0.000</td> <td>   -0.365</td> <td>   -0.145</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   No. Observations:                 6098\n",
       "Model:                          Logit   Df Residuals:                     6092\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Fri, 26 Mar 2021   Pseudo R-squ.:                 0.05637\n",
       "Time:                        12:39:24   Log-Likelihood:                -3988.5\n",
       "converged:                       True   LL-Null:                       -4226.8\n",
       "Covariance Type:            nonrobust   LLR p-value:                9.155e-101\n",
       "====================================================================================\n",
       "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "net_pay              1.1469      0.072     15.976      0.000       1.006       1.288\n",
       "type_RN             -3.4525      0.251    -13.754      0.000      -3.944      -2.960\n",
       "type_LVN+LPN        -1.7137      0.129    -13.324      0.000      -1.966      -1.462\n",
       "segmentName_d        0.5380      0.041     12.988      0.000       0.457       0.619\n",
       "prev_CW/SA_rate      0.1147      0.025      4.576      0.000       0.066       0.164\n",
       "areaName_houston    -0.2550      0.056     -4.563      0.000      -0.365      -0.145\n",
       "====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 1317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights = {0:1, 1:1.3}\n",
    "logit = LogisticRegression(solver = 'lbfgs', class_weight = weights)\n",
    "logit.fit(X_train,y_train)\n",
    "\n",
    "# logit summary\n",
    "smlogit = sm.Logit(y_train,X_train).fit()\n",
    "smlogit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "# categories = ['Zero', 'One']\n",
    "# make_confusion_matrix(confusion_matrix(y_test, y_pred), \n",
    "#                       group_names=labels,\n",
    "#                       categories=categories, \n",
    "#                       cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 572  732]\n",
      " [ 285 1025]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.44      0.53      1304\n",
      "           1       0.58      0.78      0.67      1310\n",
      "\n",
      "    accuracy                           0.61      2614\n",
      "   macro avg       0.63      0.61      0.60      2614\n",
      "weighted avg       0.63      0.61      0.60      2614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred= logit.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1312 1740]\n",
      " [ 659 2387]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.43      0.52      3052\n",
      "           1       0.58      0.78      0.67      3046\n",
      "\n",
      "    accuracy                           0.61      6098\n",
      "   macro avg       0.62      0.61      0.59      6098\n",
      "weighted avg       0.62      0.61      0.59      6098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred= logit.predict(X_train)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green> Validation set result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[676 308]\n",
      " [  1  15]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.69      0.81       984\n",
      "           1       0.05      0.94      0.09        16\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.52      0.81      0.45      1000\n",
      "weighted avg       0.98      0.69      0.80      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limiter = .6\n",
    "\n",
    "y_prob = list(logit.predict_proba(x_valid)[:,1])\n",
    "y_pred = []\n",
    "count =0\n",
    "for prob in y_prob:\n",
    "    if prob >= limiter:\n",
    "        y_pred.append(1)\n",
    "        count += 1\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "        \n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cf_matrix import make_confusion_matrix\n",
    "# labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "# categories = ['Zero', 'One']\n",
    "# make_confusion_matrix(confusion_matrix(y_valid, y_pred), \n",
    "#                       group_names=labels,\n",
    "#                       categories=categories, \n",
    "#                       cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit real data in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set input\n",
    "real_X = realdata[['net_pay', 'type_RN', 'type_LVN+LPN', 'segmentName_d', 'prev_CW/SA_rate', 'areaName_houston']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concat predicted prob with data\n",
    "realdata['prob'] = list(logit.predict_proba(real_X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata.to_csv('week322_pred.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check real data in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisweek = pd.read_csv('download.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1287,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata_test = realdata[['id', 'prob']]\n",
    "thisweek_validation = thisweek.merge(realdata_test, on = 'id', how = 'left')\n",
    "thisweek_validation.dropna(subset=['prob'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prob    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thisweek_validation[['prob']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisweek_validation\n",
    "\n",
    "def CW_by_nurse(row):\n",
    "    if row['status']=='withdrawn' and row['prevStatus'] == 'confirmed':\n",
    "        if row['withdrawnInfo_value'] == 'nurse':\n",
    "            if row['CW_Time2Start_Time'] < 0 and row['CW_Time2Start_Time'] >= -24:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "thisweek_validation['target'] = thisweek_validation.apply (lambda row: CW_by_nurse(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisweek_validation = thisweek_validation[['id', 'prob', 'target', 'net_pay', 'type', 'segmentName', 'areaName']]\n",
    "\n",
    "# thisweek_validation.to_csv('week322_check.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[353 737]\n",
      " [  5  15]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.32      0.49      1090\n",
      "           1       0.02      0.75      0.04        20\n",
      "\n",
      "    accuracy                           0.33      1110\n",
      "   macro avg       0.50      0.54      0.26      1110\n",
      "weighted avg       0.97      0.33      0.48      1110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limiter = .5\n",
    "\n",
    "\n",
    "y_prob = thisweek_validation['prob']\n",
    "y_pred = []\n",
    "count =0\n",
    "for prob in y_prob:\n",
    "    if prob >= limiter:\n",
    "        y_pred.append(1)\n",
    "        count += 1\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "        \n",
    "print(confusion_matrix(thisweek_validation['target'], y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(thisweek_validation['target'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "      <th>target</th>\n",
       "      <th>net_pay</th>\n",
       "      <th>type</th>\n",
       "      <th>segmentName</th>\n",
       "      <th>areaName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>170132</td>\n",
       "      <td>0.862836</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>LVN</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>San Antonio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>164683</td>\n",
       "      <td>0.827512</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>164684</td>\n",
       "      <td>0.821838</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>170335</td>\n",
       "      <td>0.818625</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>STNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Northeast Ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>170159</td>\n",
       "      <td>0.707361</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>DFW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>169819</td>\n",
       "      <td>0.674967</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>LPN</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Northeast Ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>170381</td>\n",
       "      <td>0.614154</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>San Antonio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>170012</td>\n",
       "      <td>0.593766</td>\n",
       "      <td>1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>STNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Northeast Ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>170425</td>\n",
       "      <td>0.578800</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>San Antonio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>167986</td>\n",
       "      <td>0.569924</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>San Antonio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>169173</td>\n",
       "      <td>0.556254</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>CMA/Med-Tech</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>169816</td>\n",
       "      <td>0.549823</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>DFW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>169670</td>\n",
       "      <td>0.536206</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>LPN</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Northeast Ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>170447</td>\n",
       "      <td>0.535148</td>\n",
       "      <td>1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>DFW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>168919</td>\n",
       "      <td>0.522320</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>170144</td>\n",
       "      <td>0.496532</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>169639</td>\n",
       "      <td>0.478606</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>LVN</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>168656</td>\n",
       "      <td>0.472515</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>169665</td>\n",
       "      <td>0.455371</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>CNA</td>\n",
       "      <td>Senior Living</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>169588</td>\n",
       "      <td>0.447668</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>RN</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      prob  target  net_pay          type    segmentName  \\\n",
       "1244  170132  0.862836       1     35.0           LVN  Senior Living   \n",
       "589   164683  0.827512       1     23.0           CNA  Senior Living   \n",
       "591   164684  0.821838       1     23.0           CNA  Senior Living   \n",
       "125   170335  0.818625       1     24.0          STNA  Senior Living   \n",
       "1170  170159  0.707361       1     18.5           CNA  Senior Living   \n",
       "954   169819  0.674967       1     39.0           LPN  Senior Living   \n",
       "1340  170381  0.614154       1     18.0           CNA  Senior Living   \n",
       "345   170012  0.593766       1     19.5          STNA  Senior Living   \n",
       "1342  170425  0.578800       1     18.5           CNA  Senior Living   \n",
       "603   167986  0.569924       1     21.0           CNA  Senior Living   \n",
       "761   169173  0.556254       1     20.0  CMA/Med-Tech  Senior Living   \n",
       "1147  169816  0.549823       1     18.5           CNA  Senior Living   \n",
       "332   169670  0.536206       1     31.0           LPN  Senior Living   \n",
       "1166  170447  0.535148       1     18.5           CNA  Senior Living   \n",
       "827   168919  0.522320       1     17.0           CNA  Senior Living   \n",
       "1231  170144  0.496532       1     19.0           CNA  Senior Living   \n",
       "1032  169639  0.478606       1     32.0           LVN  Senior Living   \n",
       "789   168656  0.472515       1     15.0           CNA  Senior Living   \n",
       "1011  169665  0.455371       1     17.0           CNA  Senior Living   \n",
       "513   169588  0.447668       1     54.0            RN     Healthcare   \n",
       "\n",
       "            areaName  \n",
       "1244     San Antonio  \n",
       "589           Austin  \n",
       "591           Austin  \n",
       "125   Northeast Ohio  \n",
       "1170             DFW  \n",
       "954   Northeast Ohio  \n",
       "1340     San Antonio  \n",
       "345   Northeast Ohio  \n",
       "1342     San Antonio  \n",
       "603      San Antonio  \n",
       "761          Houston  \n",
       "1147             DFW  \n",
       "332   Northeast Ohio  \n",
       "1166             DFW  \n",
       "827          Houston  \n",
       "1231         Houston  \n",
       "1032         Houston  \n",
       "789          Houston  \n",
       "1011         Houston  \n",
       "513          Houston  "
      ]
     },
     "execution_count": 1295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thisweek_validation[thisweek_validation['target']==1].sort_values(by = 'prob', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.547651\n",
       "1    0.606011\n",
       "Name: prob, dtype: float64"
      ]
     },
     "execution_count": 1296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thisweek_validation.groupby(\"target\")['prob'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
