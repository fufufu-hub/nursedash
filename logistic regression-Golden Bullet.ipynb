{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('model_data.csv').drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'user_id', 'shift_id', 'prev_CW/SA_rate', 'status',\n",
       "       'S_create2SA_Create', 'S_Create2Start_Time', 'SA_Create2Start_Time',\n",
       "       'U_create2now', 'U_approve2now', 'prev_CW x SA_rate', 'type_RN',\n",
       "       'type_LVN+LPN', 'segmentName_d', 'areaName_houston', 'areaName_no',\n",
       "       'areaName_dfw', 'areaName_austin', 'areaName_san', 'net_pay', 'target',\n",
       "       'sa_create', 'Start_Time', 'CW_in_a_month', 'count_prev_SA',\n",
       "       'count_prev_CW', 'f_highrate', 'f_lowrate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardlize, dont standardlize dummy! \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.drop(['id', 'user_id', 'shift_id', 'status', 'target', 'sa_create', 'Start_Time', 'type_RN', \n",
    "                    'type_LVN+LPN', 'segmentName_d', 'areaName_houston', 'areaName_no', 'areaName_dfw', \n",
    "                    'areaName_austin', 'areaName_san', 'CW_in_a_month','f_highrate','f_lowrate'], axis=1))\n",
    "\n",
    "scaled_features = scaler.transform(df.drop(['id', 'user_id', 'shift_id', 'status', 'target', 'sa_create',\n",
    "                                            'Start_Time', 'type_RN', 'type_LVN+LPN', 'segmentName_d', \n",
    "                                            'areaName_houston', 'areaName_no', 'areaName_dfw', \n",
    "                                            'areaName_austin', 'areaName_san', 'CW_in_a_month',\n",
    "                                           'f_highrate','f_lowrate'], axis=1))\n",
    "\n",
    "# scaled features\n",
    "X = pd.DataFrame(scaled_features, columns = ['prev_CW/SA_rate', 'prev_CW x SA_rate', 'S_create2SA_Create', \n",
    "                                             'S_Create2Start_Time', 'SA_Create2Start_Time', 'U_create2now', \n",
    "                                             'U_approve2now', 'net_pay', 'count_prev_SA', 'count_prev_CW'])\n",
    "# concat with dummy\n",
    "df = pd.concat([df[['id', 'user_id', 'shift_id', 'status', 'target', 'sa_create',\n",
    "                    'Start_Time', 'type_RN', 'type_LVN+LPN', 'segmentName_d', \n",
    "                    'areaName_houston', 'areaName_no', 'areaName_dfw', \n",
    "                    'areaName_austin', 'areaName_san', 'CW_in_a_month',\n",
    "                   'f_highrate','f_lowrate']], X], axis = 1)\n",
    "# drop nas\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice df by the end of this week, for predcition output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_week = '2021-4-14'\n",
    "\n",
    "# convert to datetime for conditonal selection\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n",
    "# sort by start time -> for slicing\n",
    "df = df.sort_values(by = 'Start_Time') \n",
    "# record as realdata\n",
    "realdata = df[df['Start_Time'].apply(lambda x: x > pd.to_datetime(end_of_week))]\n",
    "# record predction output rows, don't include it in tran test validation\n",
    "realdata_len = realdata.shape[0]\n",
    "# only keep status = confirmed\n",
    "realdata = realdata[realdata['status'] == 'confirmed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = green> Validation set: 1000 recently records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    902\n",
       "1     98\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slice, dont include realdata\n",
    "validation = df[-1000-realdata_len : -realdata_len]\n",
    "\n",
    "y_valid = validation['target']\n",
    "x_valid = validation.drop(['id','user_id', 'shift_id', 'status', 'sa_create', 'Start_Time', 'target'], axis = 1)\n",
    "\n",
    "y_valid.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test: main dataset - validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:-1000-realdata_len] # slice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['id','user_id', 'shift_id', 'status', 'target', 'sa_create', 'Start_Time'], axis = 1)\n",
    "y = df['target']\n",
    "\n",
    "# set test, train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight={0: 1, 1: 10}, max_iter=100000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "# assign less punlishment for classifying 0 as 1 -> find more 1's\n",
    "weights = {0:1, 1:10}\n",
    "# class_weight = 'balanced': automatically adjust weights inversely proportional to class frequencies in the input data\n",
    "\n",
    "logit = LogisticRegression(solver = 'lbfgs', max_iter=100000, class_weight = weights)\n",
    "logit.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal limiters immidiately after we create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.441296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "\n",
    "# predict probabilities\n",
    "yhat = logit.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = yhat[:, 1]\n",
    "\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_test,yhat)\n",
    "\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "\n",
    "# locate the index of the largest g-mean\n",
    "ix = argmax(gmeans)\n",
    "\n",
    "lower_limiter = thresholds[ix]\n",
    "print('Best Threshold=%f' % (lower_limiter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold=0.574\n"
     ]
    }
   ],
   "source": [
    "# search thresholds for imbalanced classification\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import f1_score\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "# predict probabilities\n",
    "yhat = logit.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = yhat[:, 1]\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "\n",
    "higher_limiter = thresholds[ix]\n",
    "\n",
    "print('Best threshold=%.3f' % (higher_limiter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13766  4218]\n",
      " [  625   784]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.77      0.85     17984\n",
      "           1       0.16      0.56      0.24      1409\n",
      "\n",
      "    accuracy                           0.75     19393\n",
      "   macro avg       0.56      0.66      0.55     19393\n",
      "weighted avg       0.90      0.75      0.81     19393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = logit.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.240541\n",
      "         Iterations 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>target</td>      <th>  No. Observations:  </th>  <td> 45250</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td> 45230</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 14 Apr 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.08089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>18:18:31</td>     <th>  Log-Likelihood:    </th> <td> -10884.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -11842.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_RN</th>              <td>   -3.5155</td> <td>    0.171</td> <td>  -20.502</td> <td> 0.000</td> <td>   -3.852</td> <td>   -3.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_LVN+LPN</th>         <td>   -1.4427</td> <td>    0.078</td> <td>  -18.460</td> <td> 0.000</td> <td>   -1.596</td> <td>   -1.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>segmentName_d</th>        <td>    0.6055</td> <td>    0.098</td> <td>    6.191</td> <td> 0.000</td> <td>    0.414</td> <td>    0.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_houston</th>     <td>   -2.9285</td> <td>    0.092</td> <td>  -31.868</td> <td> 0.000</td> <td>   -3.109</td> <td>   -2.748</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_no</th>          <td>   -2.9129</td> <td>    0.102</td> <td>  -28.625</td> <td> 0.000</td> <td>   -3.112</td> <td>   -2.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_dfw</th>         <td>   -2.8712</td> <td>    0.106</td> <td>  -26.997</td> <td> 0.000</td> <td>   -3.080</td> <td>   -2.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_austin</th>      <td>   -2.7844</td> <td>    0.125</td> <td>  -22.206</td> <td> 0.000</td> <td>   -3.030</td> <td>   -2.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>areaName_san</th>         <td>   -2.6568</td> <td>    0.131</td> <td>  -20.320</td> <td> 0.000</td> <td>   -2.913</td> <td>   -2.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CW_in_a_month</th>        <td>    0.1780</td> <td>    0.045</td> <td>    3.923</td> <td> 0.000</td> <td>    0.089</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>f_highrate</th>           <td>    0.4688</td> <td>    0.065</td> <td>    7.227</td> <td> 0.000</td> <td>    0.342</td> <td>    0.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>f_lowrate</th>            <td>   -1.2234</td> <td>    0.264</td> <td>   -4.642</td> <td> 0.000</td> <td>   -1.740</td> <td>   -0.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prev_CW/SA_rate</th>      <td>    0.1406</td> <td>    0.015</td> <td>    9.593</td> <td> 0.000</td> <td>    0.112</td> <td>    0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prev_CW x SA_rate</th>    <td>    0.1020</td> <td> 7.79e+05</td> <td> 1.31e-07</td> <td> 1.000</td> <td>-1.53e+06</td> <td> 1.53e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>S_create2SA_Create</th>   <td>   -0.1218</td> <td> 1.18e+06</td> <td>-1.03e-07</td> <td> 1.000</td> <td>-2.32e+06</td> <td> 2.32e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>S_Create2Start_Time</th>  <td>   -0.2541</td> <td>  8.8e+05</td> <td>-2.89e-07</td> <td> 1.000</td> <td>-1.73e+06</td> <td> 1.73e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SA_Create2Start_Time</th> <td>    0.0627</td> <td>    0.056</td> <td>    1.120</td> <td> 0.263</td> <td>   -0.047</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_create2now</th>         <td>   -0.1851</td> <td>    0.057</td> <td>   -3.271</td> <td> 0.001</td> <td>   -0.296</td> <td>   -0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_approve2now</th>        <td>    0.1150</td> <td>    0.031</td> <td>    3.671</td> <td> 0.000</td> <td>    0.054</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>net_pay</th>              <td>    1.0204</td> <td>    0.041</td> <td>   24.703</td> <td> 0.000</td> <td>    0.939</td> <td>    1.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>count_prev_SA</th>        <td>   -0.1976</td> <td>    0.036</td> <td>   -5.477</td> <td> 0.000</td> <td>   -0.268</td> <td>   -0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>count_prev_CW</th>        <td>    0.1362</td> <td>    0.028</td> <td>    4.836</td> <td> 0.000</td> <td>    0.081</td> <td>    0.191</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   No. Observations:                45250\n",
       "Model:                          Logit   Df Residuals:                    45230\n",
       "Method:                           MLE   Df Model:                           19\n",
       "Date:                Wed, 14 Apr 2021   Pseudo R-squ.:                 0.08089\n",
       "Time:                        18:18:31   Log-Likelihood:                -10884.\n",
       "converged:                       True   LL-Null:                       -11842.\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "========================================================================================\n",
       "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "type_RN                 -3.5155      0.171    -20.502      0.000      -3.852      -3.179\n",
       "type_LVN+LPN            -1.4427      0.078    -18.460      0.000      -1.596      -1.289\n",
       "segmentName_d            0.6055      0.098      6.191      0.000       0.414       0.797\n",
       "areaName_houston        -2.9285      0.092    -31.868      0.000      -3.109      -2.748\n",
       "areaName_no             -2.9129      0.102    -28.625      0.000      -3.112      -2.713\n",
       "areaName_dfw            -2.8712      0.106    -26.997      0.000      -3.080      -2.663\n",
       "areaName_austin         -2.7844      0.125    -22.206      0.000      -3.030      -2.539\n",
       "areaName_san            -2.6568      0.131    -20.320      0.000      -2.913      -2.401\n",
       "CW_in_a_month            0.1780      0.045      3.923      0.000       0.089       0.267\n",
       "f_highrate               0.4688      0.065      7.227      0.000       0.342       0.596\n",
       "f_lowrate               -1.2234      0.264     -4.642      0.000      -1.740      -0.707\n",
       "prev_CW/SA_rate          0.1406      0.015      9.593      0.000       0.112       0.169\n",
       "prev_CW x SA_rate        0.1020   7.79e+05   1.31e-07      1.000   -1.53e+06    1.53e+06\n",
       "S_create2SA_Create      -0.1218   1.18e+06  -1.03e-07      1.000   -2.32e+06    2.32e+06\n",
       "S_Create2Start_Time     -0.2541    8.8e+05  -2.89e-07      1.000   -1.73e+06    1.73e+06\n",
       "SA_Create2Start_Time     0.0627      0.056      1.120      0.263      -0.047       0.172\n",
       "U_create2now            -0.1851      0.057     -3.271      0.001      -0.296      -0.074\n",
       "U_approve2now            0.1150      0.031      3.671      0.000       0.054       0.176\n",
       "net_pay                  1.0204      0.041     24.703      0.000       0.939       1.101\n",
       "count_prev_SA           -0.1976      0.036     -5.477      0.000      -0.268      -0.127\n",
       "count_prev_CW            0.1362      0.028      4.836      0.000       0.081       0.191\n",
       "========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logit summary\n",
    "import statsmodels.api as sm\n",
    "smlogit = sm.Logit(y_train,X_train).fit()\n",
    "smlogit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31986  9954]\n",
      " [ 1454  1856]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85     41940\n",
      "           1       0.16      0.56      0.25      3310\n",
      "\n",
      "    accuracy                           0.75     45250\n",
      "   macro avg       0.56      0.66      0.55     45250\n",
      "weighted avg       0.90      0.75      0.80     45250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_train)\n",
    "\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36192  5748]\n",
      " [ 1991  1319]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90     41940\n",
      "           1       0.19      0.40      0.25      3310\n",
      "\n",
      "    accuracy                           0.83     45250\n",
      "   macro avg       0.57      0.63      0.58     45250\n",
      "weighted avg       0.89      0.83      0.86     45250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test threshold\n",
    "limiter = higher_limiter\n",
    "\n",
    "y_prob = list(logit.predict_proba(X_train)[:,1])\n",
    "y_pred = []\n",
    "count =0\n",
    "for prob in y_prob:\n",
    "    if prob >= limiter:\n",
    "        y_pred.append(1)\n",
    "        count+=1\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green> Validation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[671 231]\n",
      " [ 37  61]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.74      0.83       902\n",
      "           1       0.21      0.62      0.31        98\n",
      "\n",
      "    accuracy                           0.73      1000\n",
      "   macro avg       0.58      0.68      0.57      1000\n",
      "weighted avg       0.88      0.73      0.78      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test threshold\n",
    "limiter = higher_limiter\n",
    "\n",
    "y_prob = list(logit.predict_proba(x_valid)[:,1])\n",
    "y_pred = []\n",
    "count =0\n",
    "for prob in y_prob:\n",
    "    if prob >= limiter:\n",
    "        y_pred.append(1)\n",
    "        count+=1\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "\n",
    "print(confusion_matrix(y_valid, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The limiter we adopt is 0.57\n",
      "By covering 0.292 labeled as high probability of UCW, we have prepared for 0.622 of real UCW\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "label_coverage = y_pred.count(1)/len(y_pred)\n",
    "UCW_coverage = recall_score(y_valid, y_pred)\n",
    "\n",
    "print('The limiter we adopt is %.2f' % (limiter))\n",
    "print('By covering %.3f labeled as high probability of UCW, we have prepared for %.3f of real UCW' \n",
    "      % (label_coverage,UCW_coverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regressionï¼ˆtrained for houston/northeast ohio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color = green> Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_valid = validation['target']\n",
    "# x_valid = validation[['type_RN', 'type_LVN+LPN', 'areaName_houston', 'areaName_no', 'areaName_dfw',\n",
    "#                       'areaName_austin', 'areaName_san', 'net_pay',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[['type_RN', 'type_LVN+LPN', 'areaName_houston', 'areaName_no', \n",
    "#         'areaName_dfw','areaName_austin', 'areaName_san', 'net_pay',]]\n",
    "# Y = df['target']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit real data in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set input\n",
    "real_X = realdata.drop(['id','user_id', 'shift_id', 'status', 'target', 'sa_create', 'Start_Time'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['type_RN', 'type_LVN+LPN', 'segmentName_d', 'areaName_houston',\n",
       "       'areaName_no', 'areaName_dfw', 'areaName_austin', 'areaName_san',\n",
       "       'CW_in_a_month', 'f_highrate', 'f_lowrate', 'prev_CW/SA_rate',\n",
       "       'prev_CW x SA_rate', 'S_create2SA_Create', 'S_Create2Start_Time',\n",
       "       'SA_Create2Start_Time', 'U_create2now', 'U_approve2now', 'net_pay',\n",
       "       'count_prev_SA', 'count_prev_CW'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['type_RN', 'type_LVN+LPN', 'segmentName_d', 'areaName_houston',\n",
       "       'areaName_no', 'areaName_dfw', 'areaName_austin', 'areaName_san',\n",
       "       'CW_in_a_month', 'f_highrate', 'f_lowrate', 'prev_CW/SA_rate',\n",
       "       'prev_CW x SA_rate', 'S_create2SA_Create', 'S_Create2Start_Time',\n",
       "       'SA_Create2Start_Time', 'U_create2now', 'U_approve2now', 'net_pay',\n",
       "       'count_prev_SA', 'count_prev_CW'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concat predicted prob with data\n",
    "realdata['prob'] = list(logit.predict_proba(real_X)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record when this prediction is ran\n",
    "from datetime import date\n",
    "time = str(date.today().year) + '-' + str(date.today().month) + '-' + str(date.today().day)\n",
    "\n",
    "realdata[['id', 'Start_Time', 'prob']].to_csv('pred_{}_Golden_Bullet.csv'.format(time), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66040   2021-04-14 05:00:00\n",
       "66152   2021-04-14 05:00:00\n",
       "63565   2021-04-14 05:00:00\n",
       "66503   2021-04-14 05:00:00\n",
       "52308   2021-04-14 05:00:00\n",
       "                ...        \n",
       "66809   2021-05-22 06:30:00\n",
       "42945   2021-05-23 06:30:00\n",
       "42946   2021-05-24 06:30:00\n",
       "66810   2021-05-25 06:30:00\n",
       "42947   2021-06-01 06:30:00\n",
       "Name: Start_Time, Length: 812, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make the prediction doesn't include today\n",
    "realdata['Start_Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
