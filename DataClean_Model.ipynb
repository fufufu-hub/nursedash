{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB, Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# specify connection to database\n",
    "import psycopg2\n",
    "connection = psycopg2.connect(\n",
    "    host=\"nursedash-prod.cuzi2kducsnv.us-east-1.rds.amazonaws.com\",\n",
    "    database=\"nursedash\",\n",
    "    user=\"external_analyst\",\n",
    "    password=\"vWHYpF9CNtC9KWBG7sZ5JvX9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green> all time to chicago time, No withdrawn info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"\n",
    "\n",
    "SELECT  sa.id, sa.user_id, sa.shift_id, f.id AS facility_id, sa.\"withdrawnInfo\" -> 'initiator' as withdrawnInfo_value,\n",
    "sa.\"status\", sa.\"prevStatus\", \"s\".\"status\" AS \"s_status\", s.\"facility_id\", \"s\".\"description\" AS \"shift_description\",\n",
    "\"s\".\"assigned_nurse_id\", s.\"net_pay\", \"s\".\"unit\" AS \"s_unit\",s.\"type\", sa.\"hasNurseCheckEvent\",\n",
    "\"s\".\"qualifications\" AS \"s_qualifications\", \"s\".\"breakTime\" AS \"s_breakTime\", sa.\"withdrawnInfo\",\n",
    "\"f\".\"name\" AS \"f_name\", f.\"segmentName\", f.\"areaName\", \n",
    "timezone('America/Chicago', t.\"currentCheckInTime\") as t_checkin,\n",
    "timezone('America/Chicago', sa.\"createdAt\") as sa_create,\n",
    "timezone('America/Chicago', u.\"approvedAt\") as u_approve,\n",
    "timezone('America/Chicago', u.\"createdAt\") as u_create,\n",
    "timezone('America/Chicago', sa.\"statusUpdatedAt\") as sa_statusUpdate,\n",
    "timezone('America/Chicago', timezone('UTC', s.start_time)) AS \"Start_Time\" \n",
    "FROM shifts s\n",
    "Left JOIN \"TimeChangeRequests\" t ON t.\"shiftId\" = s.id\n",
    "INNER JOIN shift_applications sa ON s.id = sa.shift_id\n",
    "INNER JOIN facilities f ON s.facility_id = f.id\n",
    "INNER JOIN users u ON sa.user_id = u.id\n",
    "\n",
    "\"\"\", con = connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green> these features was done in sql before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U_create2U_approve, in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "df['U_create2U_approve'] = df.apply(lambda row: (row['u_approve'] - row['u_create']).total_seconds()/3600, \n",
    "                                    axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U_approve2now, in month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['U_approve2now'] = df.apply(lambda row: (datetime.now() - row['u_approve']).total_seconds()/2629746, \n",
    "                               axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CW_Time2Start_Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CW_Time2Start_Time\n",
    "df['CW_Time2Start_Time'] = df.apply(lambda row: (row['Start_Time'] - row['sa_statusupdate']).total_seconds()/3600 if str(row['Start_Time'])[:2] != '00' else 9999, \n",
    "                                         axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkin time 2Start_Time, In minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['checkin_Time2Start_Time'] = df.apply(lambda row: (row['Start_Time'] - row['t_checkin']).total_seconds()/60 if str(row['Start_Time'])[:2] != '00' else 99, \n",
    "                                         axis = 1)\n",
    "# # fill na as 0\n",
    "# df['checkin_Time2Start_Time'] = df['checkin_Time2Start_Time'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by time that shift was created\n",
    "df = df.sort_values(by = 'sa_create').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target: urgent withdrawn as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_by_nurse(row):\n",
    "    if row['status']=='withdrawn' and row['prevStatus'] == 'confirmed':\n",
    "        if row['withdrawninfo_value'] == 'nurse':\n",
    "            if row['CW_Time2Start_Time'] <= 24:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df['target'] = df.apply (lambda row: CW_by_nurse(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    195581\n",
       "1      6162\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find extreme UCW rate facility from Top 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the TOP 30 facilities, Ohio Living Breckenridge Village,The Weils and Park Manor of South Belt has the highest UCW rate.\n",
    "\n",
    "Whileas, The Hallmark and Village of Meyerland has the lowest UCW rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['f_highrate'] = df.apply(lambda row:1 if row['f_name'] in ['Ohio Living Breckenridge Village',\n",
    "                                                              'The Weils', \n",
    "                                                              'Park Manor of South Belt'] else 0, axis = 1)\n",
    "df['f_lowrate'] = df.apply(lambda row:1 if row['f_name'] in ['The Hallmark', \n",
    "                                                              'Village of Meyerland'] else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Count previsous shift application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track by using dictionary\n",
    "count_prev_SA = []\n",
    "test = df[['user_id', 'sa_create']]\n",
    "\n",
    "# create SA dictionary, set all value = 0 -> dramatically reduce computational cost\n",
    "uid_library = list(pd.unique(test['user_id']))\n",
    "sa_dict = {} \n",
    "for uid in uid_library:\n",
    "    sa_dict.update({uid: 0}) \n",
    "    \n",
    "for i, v in enumerate(test['user_id']):\n",
    "    sa_dict[v] += 1\n",
    "    count_prev_SA.append(sa_dict[v]-1)\n",
    " \n",
    "df['count_prev_SA'] = count_prev_SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Count previsous urgent withdrawns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track by using dictionary\n",
    "count_prev_CW = []\n",
    "test = df[['user_id', 'count_prev_SA', 'sa_create', 'target']]\n",
    "\n",
    "# create cw dictionary, set all value = 0 -> dramatically reduce computational cost\n",
    "uid_library = list(pd.unique(test['user_id']))\n",
    "cw_dict = {} \n",
    "for uid in uid_library:\n",
    "    cw_dict.update({uid: 0}) \n",
    "    \n",
    "# fill dictionary and fill cw count\n",
    "for i, v in enumerate(test['user_id']):\n",
    "    if test['target'][i] == 1:\n",
    "        cw_dict[v] += 1\n",
    "        count_prev_CW.append(cw_dict[v]-1)\n",
    "    else:\n",
    "        count_prev_CW.append(cw_dict[v])\n",
    "        \n",
    "df['count_prev_CW'] = count_prev_CW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count previsous urgent withdrawns/Count previsous shift applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prev_CW/SA_rate'] = df['count_prev_CW']/df['count_prev_SA']\n",
    "\n",
    "# fill nan with 0, happend bc 0/0, meaning rate = 0\n",
    "df['prev_CW/SA_rate'] = df['prev_CW/SA_rate'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous withdrawn times previous apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prev_CW x SA_rate'] = df['count_prev_CW']*df['count_prev_SA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Type dummy: RN, LVN + LPN , Rest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type_RN'] = df.apply(lambda row:1 if row['type'] == 'RN' else 0, axis = 1)\n",
    "df['type_LVN+LPN'] = df.apply(lambda row: 1 if row['type'] == 'LVN' or row['type'] == 'LPN' else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type_STNA'] = df.apply(lambda row:1 if row['type'] == 'STNA' else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type_CNA'] = df.apply(lambda row:1 if row['type'] == 'CNA' else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 SegementName dummy: Senior Living, Healthcare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segmentName_dummy(row):\n",
    "    if row['segmentName']=='Senior Living':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df['segmentName_d'] = df.apply(lambda row: create_segmentName_dummy(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Facility Area Name Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df[['areaName']])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-4a007819ee95>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['count'] = np.ones(len(test))\n"
     ]
    }
   ],
   "source": [
    "test = df[['id', 'user_id', 'CW_Time2Start_Time', 'target',\n",
    "           'status','prevStatus', 'withdrawninfo_value', \n",
    "           'checkin_Time2Start_Time', 's_status', 'Start_Time']]\n",
    "test['count'] = np.ones(len(test))\n",
    "# sort by start time and user id\n",
    "test = test.sort_values(by = ['user_id', 'Start_Time'], ascending = True).reset_index(drop = True)\n",
    "\n",
    "# create realibilty score dictionary\n",
    "uid_library = list(pd.unique(test['user_id']))\n",
    "Rscore_dict = {}\n",
    "for uid in uid_library:\n",
    "    Rscore_dict.update({uid:40})\n",
    "    \n",
    "# create Rs list to join with dataframe\n",
    "RS_list = []\n",
    "# fill Rs list\n",
    "for i, v in enumerate(test['user_id']):\n",
    "    ## record RS before event happen\n",
    "    RS_list.append(Rscore_dict[v])\n",
    "    \n",
    "    ## calculate next application's RS\n",
    "    \n",
    "    # if UCW\n",
    "    if test['status'][i] == 'withdrawn' and test['prevStatus'][i] == 'confirmed':\n",
    "        # ucw < 5\n",
    "        if test['CW_Time2Start_Time'][i] < 5: \n",
    "            Rscore_dict[v] -= 40\n",
    "        # 5 < ucw < 12\n",
    "        elif test['CW_Time2Start_Time'][i] < 12:\n",
    "            Rscore_dict[v] -= 20\n",
    "        # 12 < ucw < 24\n",
    "        elif test['CW_Time2Start_Time'][i] < 24:\n",
    "            Rscore_dict[v] -= 10\n",
    "        # 24 < ucw < 48\n",
    "        elif test['CW_Time2Start_Time'][i] < 48:\n",
    "            Rscore_dict[v] -= 5\n",
    "        # ucw > 48\n",
    "        else:\n",
    "            Rscore_dict[v] -= 3\n",
    "    # if not ucw -> check if check in on time/early/late    \n",
    "    else:\n",
    "        if test['checkin_Time2Start_Time'][i] > 2:\n",
    "            Rscore_dict[v] += 10\n",
    "        else:\n",
    "            Rscore_dict[v] += 3\n",
    "            \n",
    "test['reliability_score'] = RS_list\n",
    "\n",
    "# merge with original df\n",
    "test = test[['id', 'reliability_score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 97,\n",
       " 100,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 43,\n",
       " 40,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 97,\n",
       " 100,\n",
       " 103,\n",
       " 106,\n",
       " 109,\n",
       " 112,\n",
       " 115,\n",
       " 118,\n",
       " 121,\n",
       " 124,\n",
       " 127,\n",
       " 130,\n",
       " 133,\n",
       " 136,\n",
       " 139,\n",
       " 142,\n",
       " 145,\n",
       " 148,\n",
       " 151,\n",
       " 154,\n",
       " 157,\n",
       " 160,\n",
       " 163,\n",
       " 166,\n",
       " 169,\n",
       " 172,\n",
       " 175,\n",
       " 178,\n",
       " 181,\n",
       " 184,\n",
       " 187,\n",
       " 190,\n",
       " 193,\n",
       " 196,\n",
       " 199,\n",
       " 202,\n",
       " 205,\n",
       " 208,\n",
       " 211,\n",
       " 214,\n",
       " 217,\n",
       " 220,\n",
       " 223,\n",
       " 226,\n",
       " 229,\n",
       " 232,\n",
       " 235,\n",
       " 238,\n",
       " 235,\n",
       " 238,\n",
       " 241,\n",
       " 244,\n",
       " 247,\n",
       " 250,\n",
       " 253,\n",
       " 256,\n",
       " 259,\n",
       " 262,\n",
       " 265,\n",
       " 268,\n",
       " 271,\n",
       " 274,\n",
       " 277,\n",
       " 280,\n",
       " 283,\n",
       " 286,\n",
       " 289,\n",
       " 292,\n",
       " 295,\n",
       " 298,\n",
       " 301,\n",
       " 304,\n",
       " 307,\n",
       " 310,\n",
       " 313,\n",
       " 316,\n",
       " 319,\n",
       " 322,\n",
       " 325,\n",
       " 328,\n",
       " 331,\n",
       " 334,\n",
       " 337,\n",
       " 297,\n",
       " 257,\n",
       " 260,\n",
       " 263,\n",
       " 266,\n",
       " 269,\n",
       " 272,\n",
       " 275,\n",
       " 278,\n",
       " 281,\n",
       " 284,\n",
       " 287,\n",
       " 290,\n",
       " 293,\n",
       " 296,\n",
       " 299,\n",
       " 302,\n",
       " 305,\n",
       " 308,\n",
       " 311,\n",
       " 314,\n",
       " 317,\n",
       " 320,\n",
       " 323,\n",
       " 326,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 43,\n",
       " 40,\n",
       " 43,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 97,\n",
       " 100,\n",
       " 103,\n",
       " 106,\n",
       " 109,\n",
       " 112,\n",
       " 115,\n",
       " 118,\n",
       " 121,\n",
       " 124,\n",
       " 127,\n",
       " 130,\n",
       " 133,\n",
       " 136,\n",
       " 139,\n",
       " 142,\n",
       " 145,\n",
       " 148,\n",
       " 151,\n",
       " 154,\n",
       " 157,\n",
       " 160,\n",
       " 163,\n",
       " 166,\n",
       " 169,\n",
       " 172,\n",
       " 175,\n",
       " 178,\n",
       " 181,\n",
       " 184,\n",
       " 187,\n",
       " 190,\n",
       " 193,\n",
       " 196,\n",
       " 199,\n",
       " 202,\n",
       " 205,\n",
       " 208,\n",
       " 211,\n",
       " 214,\n",
       " 217,\n",
       " 220,\n",
       " 223,\n",
       " 226,\n",
       " 229,\n",
       " 232,\n",
       " 235,\n",
       " 238,\n",
       " 241,\n",
       " 244,\n",
       " 247,\n",
       " 250,\n",
       " 253,\n",
       " 256,\n",
       " 259,\n",
       " 262,\n",
       " 265,\n",
       " 225,\n",
       " 228,\n",
       " 231,\n",
       " 234,\n",
       " 237,\n",
       " 240,\n",
       " 243,\n",
       " 246,\n",
       " 249,\n",
       " 252,\n",
       " 255,\n",
       " 258,\n",
       " 261,\n",
       " 264,\n",
       " 267,\n",
       " 270,\n",
       " 273,\n",
       " 276,\n",
       " 279,\n",
       " 269,\n",
       " 272,\n",
       " 275,\n",
       " 278,\n",
       " 281,\n",
       " 284,\n",
       " 264,\n",
       " 267,\n",
       " 270,\n",
       " 273,\n",
       " 276,\n",
       " 279,\n",
       " 282,\n",
       " 285,\n",
       " 288,\n",
       " 291,\n",
       " 294,\n",
       " 297,\n",
       " 300,\n",
       " 303,\n",
       " 306,\n",
       " 309,\n",
       " 312,\n",
       " 315,\n",
       " 318,\n",
       " 321,\n",
       " 324,\n",
       " 327,\n",
       " 330,\n",
       " 333,\n",
       " 336,\n",
       " 339,\n",
       " 342,\n",
       " 345,\n",
       " 348,\n",
       " 351,\n",
       " 354,\n",
       " 357,\n",
       " 360,\n",
       " 363,\n",
       " 366,\n",
       " 369,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 97,\n",
       " 100,\n",
       " 40,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 40,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 97,\n",
       " 100,\n",
       " 103,\n",
       " 106,\n",
       " 109,\n",
       " 112,\n",
       " 115,\n",
       " 118,\n",
       " 121,\n",
       " 124,\n",
       " 127,\n",
       " 130,\n",
       " 133,\n",
       " 136,\n",
       " 139,\n",
       " 142,\n",
       " 145,\n",
       " 148,\n",
       " 151,\n",
       " 154,\n",
       " 157,\n",
       " 160,\n",
       " 163,\n",
       " 166,\n",
       " 169,\n",
       " 172,\n",
       " 175,\n",
       " 178,\n",
       " 181,\n",
       " 184,\n",
       " 187,\n",
       " 190,\n",
       " 193,\n",
       " 196,\n",
       " 199,\n",
       " 202,\n",
       " 205,\n",
       " 208,\n",
       " 211,\n",
       " 214,\n",
       " 217,\n",
       " 220,\n",
       " 223,\n",
       " 226,\n",
       " 229,\n",
       " 232,\n",
       " 235,\n",
       " 238,\n",
       " 241,\n",
       " 244,\n",
       " 247,\n",
       " 250,\n",
       " 253,\n",
       " 256,\n",
       " 259,\n",
       " 262,\n",
       " 265,\n",
       " 268,\n",
       " 271,\n",
       " 274,\n",
       " 277,\n",
       " 280,\n",
       " 283,\n",
       " 286,\n",
       " 289,\n",
       " 292,\n",
       " 295,\n",
       " 298,\n",
       " 301,\n",
       " 304,\n",
       " 307,\n",
       " 310,\n",
       " 313,\n",
       " 316,\n",
       " 319,\n",
       " 322,\n",
       " 325,\n",
       " 328,\n",
       " 331,\n",
       " 334,\n",
       " 337,\n",
       " 340,\n",
       " 343,\n",
       " 346,\n",
       " 349,\n",
       " 352,\n",
       " 355,\n",
       " 358,\n",
       " 361,\n",
       " 364,\n",
       " 367,\n",
       " 370,\n",
       " 373,\n",
       " 376,\n",
       " 379,\n",
       " 382,\n",
       " 385,\n",
       " 388,\n",
       " 391,\n",
       " 394,\n",
       " 397,\n",
       " 400,\n",
       " 403,\n",
       " 406,\n",
       " 409,\n",
       " 412,\n",
       " 415,\n",
       " 418,\n",
       " 421,\n",
       " 424,\n",
       " 427,\n",
       " 430,\n",
       " 433,\n",
       " 436,\n",
       " 439,\n",
       " 442,\n",
       " 445,\n",
       " 448,\n",
       " 451,\n",
       " 454,\n",
       " 457,\n",
       " 460,\n",
       " 463,\n",
       " 453,\n",
       " 456,\n",
       " 436,\n",
       " 439,\n",
       " 442,\n",
       " 445,\n",
       " 448,\n",
       " 451,\n",
       " 454,\n",
       " 457,\n",
       " 460,\n",
       " 463,\n",
       " 466,\n",
       " 469,\n",
       " 464,\n",
       " 467,\n",
       " 470,\n",
       " 473,\n",
       " 476,\n",
       " 479,\n",
       " 482,\n",
       " 485,\n",
       " 488,\n",
       " 491,\n",
       " 494,\n",
       " 497,\n",
       " 500,\n",
       " 503,\n",
       " 506,\n",
       " 509,\n",
       " 512,\n",
       " 515,\n",
       " 518,\n",
       " 521,\n",
       " 501,\n",
       " 496,\n",
       " 499,\n",
       " 502,\n",
       " 505,\n",
       " 508,\n",
       " 511,\n",
       " 514,\n",
       " 517,\n",
       " 520,\n",
       " 515,\n",
       " 518,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 97,\n",
       " 100,\n",
       " 103,\n",
       " 106,\n",
       " 109,\n",
       " 112,\n",
       " 115,\n",
       " 118,\n",
       " 121,\n",
       " 124,\n",
       " 127,\n",
       " 130,\n",
       " 133,\n",
       " 136,\n",
       " 139,\n",
       " 142,\n",
       " 145,\n",
       " 148,\n",
       " 151,\n",
       " 154,\n",
       " 157,\n",
       " 160,\n",
       " 163,\n",
       " 166,\n",
       " 169,\n",
       " 172,\n",
       " 175,\n",
       " 178,\n",
       " 181,\n",
       " 184,\n",
       " 187,\n",
       " 190,\n",
       " 193,\n",
       " 196,\n",
       " 199,\n",
       " 202,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 40,\n",
       " 43,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 97,\n",
       " 100,\n",
       " 103,\n",
       " 106,\n",
       " 109,\n",
       " 112,\n",
       " 115,\n",
       " 118,\n",
       " 121,\n",
       " 40,\n",
       " 40,\n",
       " 43,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 97,\n",
       " 100,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 97,\n",
       " 100,\n",
       " 103,\n",
       " 106,\n",
       " 109,\n",
       " 112,\n",
       " 115,\n",
       " 118,\n",
       " 121,\n",
       " 124,\n",
       " 127,\n",
       " 130,\n",
       " 133,\n",
       " 136,\n",
       " 139,\n",
       " 142,\n",
       " 145,\n",
       " 148,\n",
       " 151,\n",
       " 154,\n",
       " 157,\n",
       " 160,\n",
       " 163,\n",
       " 166,\n",
       " 169,\n",
       " 172,\n",
       " 175,\n",
       " 178,\n",
       " 181,\n",
       " 184,\n",
       " 187,\n",
       " 190,\n",
       " 193,\n",
       " 196,\n",
       " 199,\n",
       " 202,\n",
       " 205,\n",
       " 208,\n",
       " 211,\n",
       " 214,\n",
       " 217,\n",
       " 220,\n",
       " 223,\n",
       " 226,\n",
       " 229,\n",
       " 232,\n",
       " 235,\n",
       " 238,\n",
       " 241,\n",
       " 244,\n",
       " 247,\n",
       " 250,\n",
       " 253,\n",
       " 256,\n",
       " 259,\n",
       " 262,\n",
       " 265,\n",
       " 268,\n",
       " 271,\n",
       " 274,\n",
       " 277,\n",
       " 280,\n",
       " 283,\n",
       " 286,\n",
       " 289,\n",
       " 292,\n",
       " 295,\n",
       " 298,\n",
       " 301,\n",
       " 304,\n",
       " 307,\n",
       " 310,\n",
       " 313,\n",
       " 316,\n",
       " 319,\n",
       " 322,\n",
       " 325,\n",
       " 328,\n",
       " 331,\n",
       " 334,\n",
       " 337,\n",
       " 340,\n",
       " 343,\n",
       " 346,\n",
       " 349,\n",
       " 352,\n",
       " 355,\n",
       " 358,\n",
       " 361,\n",
       " 364,\n",
       " 367,\n",
       " 370,\n",
       " 373,\n",
       " 376,\n",
       " 379,\n",
       " 382,\n",
       " 385,\n",
       " 388,\n",
       " 391,\n",
       " 394,\n",
       " 397,\n",
       " 400,\n",
       " 403,\n",
       " 40,\n",
       " 43,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 49,\n",
       " 52,\n",
       " 55,\n",
       " 58,\n",
       " 61,\n",
       " 64,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 97,\n",
       " 100,\n",
       " 103,\n",
       " 106,\n",
       " 109,\n",
       " 112,\n",
       " 115,\n",
       " 118,\n",
       " 121,\n",
       " 124,\n",
       " 127,\n",
       " 130,\n",
       " 133,\n",
       " 136,\n",
       " 139,\n",
       " 142,\n",
       " 145,\n",
       " 148,\n",
       " 151,\n",
       " 154,\n",
       " 157,\n",
       " 160,\n",
       " 163,\n",
       " 166,\n",
       " 169,\n",
       " 172,\n",
       " 175,\n",
       " 178,\n",
       " 181,\n",
       " 184,\n",
       " 187,\n",
       " 190,\n",
       " 193,\n",
       " 196,\n",
       " 199,\n",
       " 202,\n",
       " 205,\n",
       " 208,\n",
       " 211,\n",
       " 214,\n",
       " 217,\n",
       " 220,\n",
       " 223,\n",
       " 226,\n",
       " 229,\n",
       " 232,\n",
       " 235,\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RS_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RS(row):\n",
    "    if row['reliability_score'] > 200:\n",
    "        return 200\n",
    "    elif row['reliability_score'] < 0:\n",
    "        return -100\n",
    "    else:\n",
    "        return row['reliability_score']\n",
    "test['reliability_score'] = test.apply(lambda row: RS(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    201743.000000\n",
       "mean        124.170108\n",
       "std          80.715820\n",
       "min        -100.000000\n",
       "25%          61.000000\n",
       "50%         139.000000\n",
       "75%         200.000000\n",
       "max         200.000000\n",
       "Name: reliability_score, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['reliability_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.merge(test, on = 'id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep only prevstatus or status = confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[df1.apply(lambda row: (row['prevStatus'] == 'confirmed') or (row['status'] == 'confirmed'), axis = 1)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82470, 52)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many left\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many previsous urgent withdrawns in a month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = green>output feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'user_id', 'shift_id', 'facility_id', 'withdrawninfo_value',\n",
       "       'status', 'prevStatus', 's_status', 'facility_id', 'shift_description',\n",
       "       'assigned_nurse_id', 'net_pay', 's_unit', 'type', 'hasNurseCheckEvent',\n",
       "       's_qualifications', 's_breakTime', 'withdrawnInfo', 'f_name',\n",
       "       'f_short_name', 'segmentName', 'areaName', 't_checkin', 's_create',\n",
       "       'sa_create', 'u_approve', 'u_create', 'sa_statusupdate', 'Start_Time',\n",
       "       'U_create2U_approve', 'U_approve2now', 'CW_Time2Start_Time',\n",
       "       'checkin_Time2Start_Time', 'target', 'f_highrate', 'f_lowrate',\n",
       "       'count_prev_SA', 'count_prev_CW', 'prev_CW/SA_rate',\n",
       "       'prev_CW x SA_rate', 'type_RN', 'type_LVN+LPN', 'type_STNA', 'type_CNA',\n",
       "       'segmentName_d', 'areaName_Austin', 'areaName_Cincinnati',\n",
       "       'areaName_DFW', 'areaName_Houston', 'areaName_Northeast Ohio',\n",
       "       'areaName_San Antonio', 'reliability_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = df2[['id', 'user_id', 'shift_id', 'prev_CW/SA_rate', 'status', 'U_approve2now','prev_CW x SA_rate',\n",
    "                 'type_RN', 'type_LVN+LPN', 'type_STNA', 'segmentName_d', 'areaName', 'type', \n",
    "                 'net_pay', 'target', 'sa_create', 'Start_Time', 'areaName_Austin', 'areaName_Cincinnati',\n",
    "                 'areaName_DFW', 'areaName_Houston', 'areaName_Northeast Ohio', 'areaName_San Antonio', \n",
    "                 'count_prev_SA', 'count_prev_CW','f_highrate','f_lowrate','type_CNA','reliability_score']]\n",
    "# feature_df.to_csv('model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82470, 29)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature column\n",
    "features_colname = ['net_pay', 'prev_CW/SA_rate', 'U_approve2now', \n",
    "                    'prev_CW x SA_rate', 'type_RN', 'type_LVN+LPN', 'segmentName_d',  'count_prev_SA', \n",
    "                    'count_prev_CW', 'areaName_Northeast Ohio', 'areaName_Houston',\n",
    "                    'areaName_DFW', 'areaName_Austin', 'areaName_San Antonio'] \n",
    "\n",
    "\n",
    "\n",
    "from datetime import date, timedelta\n",
    "#create a datetime object for tomorrow\n",
    "tmrrw = date.today() + timedelta(days=1)\n",
    "#create a string object for tomorrow\n",
    "tomorrow = str(tmrrw.year) + '-' + str(tmrrw.month) + '-' + str(tmrrw.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Body(preprocessing - classification report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-b5031ab03ce3>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['U_approve2now'] = df['U_approve2now'].fillna(-1)\n",
      "<ipython-input-31-b5031ab03ce3>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.253274\n",
      "         Iterations 8\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "### import data ###\n",
    "###################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# df = pd.read_csv('model_data.csv').drop(columns = ['Unnamed: 0'])\n",
    "# some values are nan, mark it as -1\n",
    "df = feature_df\n",
    "df['U_approve2now'] = df['U_approve2now'].fillna(-1)\n",
    "\n",
    "######################\n",
    "### data prepration###\n",
    "######################\n",
    "\n",
    "## output: applications after today\n",
    "\n",
    "# convert to datetime for conditonal selection\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n",
    "# sort by start time -> for slicing\n",
    "df = df.sort_values(by = 'Start_Time') \n",
    "# record as realdata\n",
    "realdata = df[df['Start_Time'].apply(lambda x: x > pd.to_datetime(tomorrow))]\n",
    "# record predction output rows, don't include it in tran test validation\n",
    "realdata_len = realdata.shape[0]\n",
    "# only keep status = confirmed\n",
    "realdata = realdata[realdata['status'] == 'confirmed']\n",
    "\n",
    "## Validation set: 1000 recently records\n",
    "# slice, dont include realdata\n",
    "validation = df[-1000-realdata_len : -realdata_len] # slice\n",
    "y_valid = validation['target'] # prep y\n",
    "x_valid = validation[features_colname] # prep x\n",
    "\n",
    "## Train test: main dataset - validation set - output(realdata) set\n",
    "traintest = df[:-1000-realdata_len] # slice \n",
    "X = traintest[features_colname] # prep x\n",
    "y = traintest['target'] # prep y\n",
    "# set test, train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "\n",
    "###################\n",
    "### train logit ###\n",
    "###################\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "# assign less punlishment for classifying 0 as 1 -> find more 1's\n",
    "weights = {0:1, 1:10}\n",
    "# class_weight = 'balanced': automatically adjust weights inversely proportional to class frequencies in the input data\n",
    "logit = LogisticRegression(solver = 'lbfgs', max_iter=100000, class_weight = weights)\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#############\n",
    "# Threshold #\n",
    "#############\n",
    "from sklearn.metrics import roc_curve\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "# predict probabilities\n",
    "yhat = logit.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = yhat[:, 1]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_test,yhat)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = argmax(gmeans)\n",
    "lower_limiter = thresholds[ix]\n",
    "\n",
    "# search thresholds for imbalanced classification\n",
    "from numpy import arange\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import f1_score\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "# predict probabilities\n",
    "yhat = logit.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = yhat[:, 1]\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(y_test, to_labels(probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "higher_limiter = thresholds[ix]\n",
    "\n",
    "\n",
    "#########################\n",
    "### train test result ###\n",
    "#########################\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = logit.predict(X_test)\n",
    "Traintest_CM = confusion_matrix(y_test, y_pred)\n",
    "Traintest_CR = classification_report(y_test, y_pred)\n",
    "\n",
    "# overfit\n",
    "y_pred = logit.predict(X_train)\n",
    "overfit_CM = confusion_matrix(y_train, y_pred)\n",
    "overfit_CR = classification_report(y_train, y_pred)\n",
    "\n",
    "# logit summary\n",
    "import statsmodels.api as sm\n",
    "smlogit = sm.Logit(y_train, X_train).fit()\n",
    "logit_summary = smlogit.summary()\n",
    "\n",
    "\n",
    "#########################\n",
    "### validation result ###\n",
    "#########################\n",
    "# lower threshold\n",
    "limiter = lower_limiter\n",
    "y_prob = list(logit.predict_proba(x_valid)[:,1])\n",
    "y_pred = []\n",
    "count =0\n",
    "for prob in y_prob:\n",
    "    if prob >= limiter:\n",
    "        y_pred.append(1)\n",
    "        count+=1\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "low_vad_CM = confusion_matrix(y_valid, y_pred)\n",
    "low_vad_CR = classification_report(y_valid, y_pred)\n",
    "Dict_low_vad_CR = classification_report(y_valid, y_pred, output_dict=True)\n",
    "\n",
    "# high threshold\n",
    "limiter = 0.55 # set to fix value\n",
    "y_prob = list(logit.predict_proba(x_valid)[:,1])\n",
    "y_pred = []\n",
    "count =0\n",
    "for prob in y_prob:\n",
    "    if prob >= limiter:\n",
    "        y_pred.append(1)\n",
    "        count+=1\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "high_vad_CM = confusion_matrix(y_valid, y_pred)\n",
    "high_vad_CR = classification_report(y_valid, y_pred)\n",
    "Dict_high_vad_CR = classification_report(y_valid, y_pred, output_dict=True)\n",
    "\n",
    "########################\n",
    "### excutive summary ###\n",
    "########################\n",
    "from sklearn.metrics import recall_score\n",
    "label_coverage = y_pred.count(1)/len(y_pred)\n",
    "UCW_coverage = recall_score(y_valid, y_pred)\n",
    "Excutive_Summary = 'The limiter we adopt is %.2f' % (limiter) + '. ' + 'By covering %.3f labeled as high probability of UCW, we have prepared for %.3f of real UCW' % (label_coverage,UCW_coverage)\n",
    "\n",
    "\n",
    "########################\n",
    "### output prediction ##\n",
    "########################\n",
    "# return lower_limiter, higher_limiter, Traintest_CM, Traintest_CR, overfit_CM, overfit_CR, \n",
    "# low_vad_CM, low_vad_CR, low_vad_CR, low_vad_CR, logit_summary, logit, Excutive_Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:                56008\n",
      "Model:                          Logit   Df Residuals:                    55994\n",
      "Method:                           MLE   Df Model:                           13\n",
      "Date:                Sun, 13 Jun 2021   Pseudo R-squ.:                 0.05468\n",
      "Time:                        16:19:13   Log-Likelihood:                -14185.\n",
      "converged:                       True   LL-Null:                       -15006.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "net_pay                     0.0776      0.003     22.607      0.000       0.071       0.084\n",
      "prev_CW/SA_rate             2.3231      0.210     11.037      0.000       1.911       2.736\n",
      "U_approve2now              -0.0264      0.003     -9.525      0.000      -0.032      -0.021\n",
      "prev_CW x SA_rate        7.322e-05   2.37e-05      3.084      0.002    2.67e-05       0.000\n",
      "type_RN                    -3.0138      0.145    -20.723      0.000      -3.299      -2.729\n",
      "type_LVN+LPN               -1.1809      0.064    -18.368      0.000      -1.307      -1.055\n",
      "segmentName_d               0.0947      0.074      1.282      0.200      -0.050       0.239\n",
      "count_prev_SA              -0.0020      0.000     -8.449      0.000      -0.003      -0.002\n",
      "count_prev_CW               0.0601      0.007      8.115      0.000       0.046       0.075\n",
      "areaName_Northeast Ohio    -3.5873      0.120    -29.966      0.000      -3.822      -3.353\n",
      "areaName_Houston           -3.5711      0.108    -33.095      0.000      -3.783      -3.360\n",
      "areaName_DFW               -3.5695      0.117    -30.526      0.000      -3.799      -3.340\n",
      "areaName_Austin            -3.6358      0.132    -27.512      0.000      -3.895      -3.377\n",
      "areaName_San Antonio       -3.4337      0.130    -26.511      0.000      -3.688      -3.180\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(logit_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16780  5358]\n",
      " [  848  1018]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.76      0.84     22138\n",
      "           1       0.16      0.55      0.25      1866\n",
      "\n",
      "    accuracy                           0.74     24004\n",
      "   macro avg       0.56      0.65      0.55     24004\n",
      "weighted avg       0.89      0.74      0.80     24004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Traintest_CM)\n",
    "print('\\n')\n",
    "print(Traintest_CR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39393 12380]\n",
      " [ 1850  2385]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85     51773\n",
      "           1       0.16      0.56      0.25      4235\n",
      "\n",
      "    accuracy                           0.75     56008\n",
      "   macro avg       0.56      0.66      0.55     56008\n",
      "weighted avg       0.90      0.75      0.80     56008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(overfit_CM)\n",
    "print('\\n')\n",
    "print(overfit_CR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "#### <font color = Blue> Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying lower(for Recall) threshhold of 0.43848450631536495\n",
      "\n",
      "\n",
      "[[399 513]\n",
      " [ 10  78]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.44      0.60       912\n",
      "           1       0.13      0.89      0.23        88\n",
      "\n",
      "    accuracy                           0.48      1000\n",
      "   macro avg       0.55      0.66      0.42      1000\n",
      "weighted avg       0.90      0.48      0.57      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Applying lower(for Recall) threshhold of {}'.format(lower_limiter))\n",
    "print('\\n')\n",
    "print(low_vad_CM)\n",
    "print('\\n')\n",
    "print(low_vad_CR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = Blue> Higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying higher(for precision) threshhold of 0.532\n",
      "\n",
      "\n",
      "[[689 223]\n",
      " [ 32  56]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.84       912\n",
      "           1       0.20      0.64      0.31        88\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.58      0.70      0.57      1000\n",
      "weighted avg       0.89      0.74      0.80      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Applying higher(for precision) threshhold of {}'.format(higher_limiter))\n",
    "print('\\n')\n",
    "print(high_vad_CM)\n",
    "print('\\n')\n",
    "print(high_vad_CR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excutive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By setting limiter to 0.438. We Can prepare for 88.64% of real UCW, by marking 56.90% labeled as low probability of UCW\n",
      "\n",
      "\n",
      "By setting limiter to 0.550. We Can prepare for 63.64% of real UCW, by marking 27.90% labeled as high probability of UCW\n"
     ]
    }
   ],
   "source": [
    "high_label_coverage = (high_vad_CM[0][1]+high_vad_CM[1][1])/1000\n",
    "high_UCW_coverage = Dict_high_vad_CR['1']['recall']\n",
    "low_label_coverage = (low_vad_CM[0][1]+high_vad_CM[1][1])/1000\n",
    "low_UCW_coverage = Dict_low_vad_CR['1']['recall']\n",
    "\n",
    "print('By setting limiter to %.3f' % (lower_limiter) + '. ' \n",
    "      +'We Can prepare for {:.2%} of real UCW, by marking {:.2%} labeled as low probability of UCW'.format(low_UCW_coverage, low_label_coverage))\n",
    "print('\\n')\n",
    "print('By setting limiter to %.3f' % (0.55) + '. ' \n",
    "      +'We Can prepare for {:.2%} of real UCW, by marking {:.2%} labeled as high probability of UCW'.format(high_UCW_coverage, high_label_coverage))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit real data in this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set input\n",
    "real_X = realdata[features_colname]\n",
    "\n",
    "# concat predicted prob with data\n",
    "realdata['prob'] = list(logit.predict_proba(real_X)[:,1])\n",
    "\n",
    "# record when this prediction is ran\n",
    "from datetime import date\n",
    "time = str(date.today().year) + '-' + str(date.today().month) + '-' + str(date.today().day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"\n",
    "\n",
    "SELECT  sa.id, sa.user_id, sa.shift_id, u.\"phone\", u.\"name\", f.id AS facility_id, sa.\"withdrawnInfo\" -> 'initiator' as withdrawnInfo_value,\n",
    "sa.\"status\", sa.\"prevStatus\", sa.\"distance\", s.\"facility_id\", \"s\".\"description\" AS \"shift_description\",\n",
    "\"s\".\"assigned_nurse_id\",s.\"type\",\n",
    "\"s\".\"qualifications\" AS \"s_qualifications\", sa.\"withdrawnInfo\",\n",
    "\"f\".\"name\" AS \"facility_name\",\"f\".\"short_name\" AS \"f_short_name\", f.\"segmentName\", f.\"areaName\", \n",
    "timezone('America/Chicago', timezone('UTC', s.start_time)) AS \"Start_Time\" \n",
    "FROM shifts s\n",
    "INNER JOIN shift_applications sa ON s.id = sa.shift_id\n",
    "INNER JOIN facilities f ON s.facility_id = f.id\n",
    "INNER JOIN users u ON sa.user_id = u.id\n",
    "\n",
    "\"\"\", con = connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_day(hour):\n",
    "    return (\n",
    "        \"morning\" if 4 < hour <= 12\n",
    "        else\n",
    "        \"afternoon\" if 12 < hour <= 17\n",
    "        else\n",
    "        \"evening/night\" if 18 < hour <= 22\n",
    "        else\n",
    "        \"overnight\"\n",
    "    )\n",
    "\n",
    "df['Start_time_of_the_day'] = df.apply(lambda row: get_part_of_day(row['Start_Time'].hour), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prediction file\n",
    "prediction = realdata[['id', 'Start_Time', 'prob']]\n",
    "validation = prediction.merge(df, on = 'id', how = 'left')\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "# convert to datetime for conditonal selection\n",
    "validation['Start_Time_x'] = pd.to_datetime(validation['Start_Time_x'])\n",
    "\n",
    "# only select date part of the time\n",
    "validation['Start_Date'] = validation.apply(lambda row: str(row['Start_Time_x'].date()), axis = 1)\n",
    "\n",
    "validation['Start_Time'] = validation.apply(lambda row: str(row['Start_Time_x'].time()), axis =1)\n",
    "\n",
    "# convert to datetime for conditonal selection\n",
    "validation['Start_Time_x'] = pd.to_datetime(validation['Start_Time_x'])\n",
    "\n",
    "# only select date part of the time\n",
    "validation['Start_Time_x'] = validation.apply(lambda row: str(row['Start_Time_x'].date()), axis = 1)\n",
    "\n",
    "# rename start time\n",
    "validation = validation.rename(columns={\"Start_Time_x\": \"Start_Time\"})\n",
    "\n",
    "# limit our result to what we want as validation file\n",
    "validation0 = validation[['id','prob','Start_Time','Start_time_of_the_day','status','type','prevStatus','areaName','segmentName','facility_name','user_id']]\n",
    "\n",
    "validation0 = validation0.set_index(\"id\")\n",
    "validation0.columns\n",
    "\n",
    "validation0.to_csv('pred_{}_Silver_Bullet.csv'.format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for houston messaging use\n",
    "validation1 = validation[(validation['areaName'] == 'Houston') & \n",
    "           (validation['type'] == 'CNA') & (validation['prob'] >= 0.47)]\n",
    "\n",
    "# limit our result to what we want as validation file\n",
    "validation1 = validation1[['id','phone','name','type','Start_Date','Start_Time','facility_name','user_id','prob']]\n",
    "\n",
    "validation1 = validation1.set_index(\"id\")\n",
    "\n",
    "validation1.to_excel(\"list_for_Houston_{}.xlsx\".format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Houston Plan2 use\n",
    "pivot_table = validation[(validation['areaName'] == 'Northeast Ohio') & \n",
    "           (validation['type'] == 'STNA') & (validation['prob'] >= 0.55)].groupby([\"Start_Date\",\n",
    "                                    \"Start_time_of_the_day\"]).size().reset_index(name='count').set_index(\"Start_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table.to_excel(\"plan2_pred_{}.xlsx\".format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
